{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/kkstream-deep-learning-workshop/overview\n",
    "\n",
    "https://github.com/KKStream/datateam_workshop_dl\n",
    "\n",
    "Join the competition and get the preprocessed dataset. Please make submissions\n",
    "as many as you can, which will be evaluated with AUC by Kaggle and revealed on\n",
    "leaderboard. There are several benchmarks on the learderboard. Each of them\n",
    "was predicted with different model. We expect you to get at least 0.82152 (\n",
    "Santino Corleone) in this assignment. After you are done, send back the report\n",
    "describing how you build your prediction model, the source code, and MD5 of\n",
    "your best submissions along with your Kaggle account for confirmation.\n",
    "As our intention is to evaluate your skill, not taking advantage of you, feel\n",
    "free to refuse providing the source code if you beat the second benchmark on\n",
    "the leaderboard (Vito Corleone, 0.89064).\n",
    "Things you can do:\n",
    "    * Implement a model and evaluate it.\n",
    "    * Implement a model from scratch (e.g. do deep learning with numpy).\n",
    "    * Extract and try different features from raw data.\n",
    "    * ...\n",
    "Note:\n",
    "    * We expect you to get at least 0.82152.\n",
    "    * We have prepare the code to explain the pre-processed dataset, feel free\n",
    "      to change/replace it.\n",
    "    * If it's done in python, please send *.py (no *.ipynb).\n",
    "    * Although this is not a coding style test, please make the code readable,\n",
    "      please..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://kobkrit.com/using-allow-growth-memory-option-in-tensorflow-and-keras-dc8c8081bc96\n",
    "# Using allow_growth memory option in Keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define build_model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # NOTE: shape of input tensors is (N, 896)\n",
    "    # TODO: change number of neurons of each layer\n",
    "    model.add(Dense(50, activation='elu', input_shape=(896,))) \n",
    "    \n",
    "    # TODO: change the activation of hidden layers and check the performance\n",
    "    #       possible value: 'elu' / 'selu' / 'tanh' / 'relu' / 'sigmoid' / ...\n",
    "    model.add(Dense(50, activation='relu', input_shape=(896,)))\n",
    "    \n",
    "    # TODO: add more dense layer and check the performance\n",
    "    # NOTE: the number of neurons of the final layer must be 28 cause we are predicting 28 values.\n",
    "    model.add(Dense(28, activation='sigmoid'))\n",
    "\n",
    "    # TODO: change the optimizer and check the performance\n",
    "    #       possible values: 'sgd' / 'adam' / ...\n",
    "\n",
    "    # TODO: change the loss function and check the performance\n",
    "    #       possible values: 'mean_squared_error' / 'binary_crossentropy' / ...\n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define write_result for submissions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(name, predictions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if predictions is None:\n",
    "        raise Exception('need predictions')\n",
    "\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "    if not os.path.exists('./results/'):\n",
    "        os.makedirs('./results/')\n",
    "\n",
    "    path = os.path.join('./results/', name)\n",
    "\n",
    "    with open(path, 'wt', encoding='utf-8', newline='') as csv_target_file:\n",
    "        target_writer = csv.writer(csv_target_file, lineterminator='\\n')\n",
    "\n",
    "        header = [\n",
    "            'user_id',\n",
    "            'time_slot_0', 'time_slot_1', 'time_slot_2', 'time_slot_3',\n",
    "            'time_slot_4', 'time_slot_5', 'time_slot_6', 'time_slot_7',\n",
    "            'time_slot_8', 'time_slot_9', 'time_slot_10', 'time_slot_11',\n",
    "            'time_slot_12', 'time_slot_13', 'time_slot_14', 'time_slot_15',\n",
    "            'time_slot_16', 'time_slot_17', 'time_slot_18', 'time_slot_19',\n",
    "            'time_slot_20', 'time_slot_21', 'time_slot_22', 'time_slot_23',\n",
    "            'time_slot_24', 'time_slot_25', 'time_slot_26', 'time_slot_27',\n",
    "        ]\n",
    "\n",
    "        target_writer.writerow(header)\n",
    "\n",
    "        for i in range(0, len(predictions), 28):\n",
    "            # NOTE: 57159 is the offset of user ids\n",
    "            userid = [57159 + i // 28]\n",
    "            labels = predictions[i:i+28].tolist()\n",
    "\n",
    "            target_writer.writerow(userid + labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from 'v0_eigens.npz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: load the data from the npz\n",
    "dataset = np.load('./datasets/v0_eigens.npz') #contains 'train_eigens' and 'issue_eigens'\n",
    "\n",
    "# NOTE: calculate the size of training set and validation set\n",
    "#       all pre-processed features are inside 'train_eigens'\n",
    "train_data_size = dataset['train_eigens'].shape[0] # train_data_size = 57159, (57159, 924)\n",
    "valid_data_size = train_data_size // 5 # valid_data_size= 11431\n",
    "train_data_size = train_data_size - valid_data_size # train_data_size= 45728\n",
    "indices = np.arange(train_data_size + valid_data_size) # indices= [    0     1     2 ... 57156 57157 57158]\n",
    "\n",
    "# NOTE: split dataset\n",
    "train_data = dataset['train_eigens'][indices[:train_data_size]] # [:45728] \n",
    "valid_data = dataset['train_eigens'][indices[train_data_size:]] # [45728:]\n",
    "\n",
    "# NOTE: a 896d feature vector for each user, the 28d vector in the end are\n",
    "#       labels\n",
    "#       896 = 32 (weeks) x 7 (days a week) x 4 (segments a day)\n",
    "# train_data.shape is (45728, 924)\n",
    "train_eigens = train_data[:, :-28] #(45728, 896)\n",
    "train_labels = train_data[:, -28:] #(45728, 28)\n",
    "valid_eigens = valid_data[:, :-28] #(11431, 896)\n",
    "valid_labels = valid_data[:, -28:] #(11431, 28)\n",
    "issue_eigens = dataset['issue_eigens'][:, :-28] #(37092, 896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_eigens.shape = (45728, 896)\n",
      "train_labels.shape = (45728, 28)\n",
      "valid_eigens.shape = (11431, 896)\n",
      "valid_labels.shape = (11431, 28)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: check the shape of the prepared dataset\n",
    "print('train_eigens.shape = {}'.format(train_eigens.shape))\n",
    "print('train_labels.shape = {}'.format(train_labels.shape))\n",
    "print('valid_eigens.shape = {}'.format(valid_eigens.shape))\n",
    "print('valid_labels.shape = {}'.format(valid_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model using build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45728 samples, validate on 11431 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6757 - val_loss: 0.6501\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6223 - val_loss: 0.5919\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.5573 - val_loss: 0.5204\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.4858 - val_loss: 0.4516\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.4246 - val_loss: 0.3980\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.3768 - val_loss: 0.3557\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.3383 - val_loss: 0.3216\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.3077 - val_loss: 0.2951\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.2845 - val_loss: 0.2756\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.2679 - val_loss: 0.2619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26641acba8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model\n",
    "model = build_model()\n",
    "\n",
    "model.fit(\n",
    "    x=train_eigens, #(45728, 896)\n",
    "    y=train_labels, #(45728, 28)\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=2,\n",
    "    validation_data=(valid_eigens, valid_labels), # ((11431, 896), (11431, 28))\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performance on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_guesss_auc = 0.8068629556004255\n"
     ]
    }
   ],
   "source": [
    "# Validate Performance on validation dataset\n",
    "def auc(guess, truth):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    guess = guess.flatten()\n",
    "    truth = truth.flatten()\n",
    "    \n",
    "    fprs, tprs, _ = sklearn.metrics.roc_curve(truth, guess)\n",
    "\n",
    "    return sklearn.metrics.auc(fprs, tprs)\n",
    "\n",
    "valid_guesss = model.predict(valid_eigens) #(11431, 896)\n",
    "# valid_guesss.shape : (11431, 28)\n",
    "\n",
    "valid_guesss_auc = auc(valid_guesss, valid_labels) #((11431, 28),(11431, 28))\n",
    "\n",
    "print ('valid_guesss_auc = {}'.format(valid_guesss_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the curve (AUC) of a receiver operating characteristic (ROC) curve is a way to reduce ROC performance to a single value representing expected performance.\n",
    "\n",
    "A ROC curve plots the true positives (sensitivity) vs. false positives (1 âˆ’ specificity), for a binary classifier system as its discrimination threshold is varied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performance on testing dataset and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if performance is good, use the model to predict labels of testing dataset\n",
    "# NOTE: predict and save\n",
    "issue_guesss = model.predict(issue_eigens) #(37092, 896)\n",
    "# issue_guesss.shape : (37092, 28)\n",
    "\n",
    "write_result('dense_test.csv', issue_guesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
